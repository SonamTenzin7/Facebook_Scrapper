# ğŸ¤– Kuensel Facebook Scraper - Cloud Automation

A fully automated Facebook scraper that collects posts from Kuensel's Facebook page and presents them through a clean API and web interface. **Now runs entirely in the cloud via GitHub Actions!**

## ğŸŒ Live Demo
- **Posts API**: [https://sonamtenzin7.github.io/Facebook_Scrapper/static_api/posts.json](https://sonamtenzin7.github.io/Facebook_Scrapper/static_api/posts.json)
- **Web Interface**: [https://sonamtenzin7.github.io/Facebook_Scrapper/](https://sonamtenzin7.github.io/Facebook_Scrapper/)

## âœ¨ Features

### ğŸš€ Cloud Automation
- âš¡ **100% cloud-based** - runs on GitHub Actions, no laptop needed
- ğŸ• **Smart scheduling** - 4 times daily at optimal times (Bhutan timezone)
- ğŸ§¹ **Automatic cleanup** - intelligent workflow management
- ğŸ“Š **Repository maintenance** - automated optimization and health checks

### ğŸ“± API & Interface
- ğŸ”— **JSON API** for easy integration with other applications
- ğŸ“Š Statistics dashboard showing total posts, images, and categories
- ğŸ” Filter posts by category and content
- ğŸ“± Responsive design for mobile and desktop
- ğŸ–¼ï¸ Image gallery with click-to-expand
- âš¡ Static file generation for lightning-fast loading

### ğŸ›¡ï¸ Reliability & Monitoring  
- ğŸ”„ **Smart retry logic** and error handling
- ğŸ“ˆ **Performance monitoring** and optimization
- ğŸ—‘ï¸ **Automatic cleanup** of old data and workflows
- ğŸ” **Secure configuration** via GitHub Secrets

## ğŸš€ Cloud Setup (Recommended)

### Prerequisites
1. Fork this repository to your GitHub account
2. Enable GitHub Actions in your repository settings
3. Enable GitHub Pages for automated deployment

### Configuration
Add your Facebook configuration as a GitHub Secret:

1. Go to **Settings** â†’ **Secrets and variables** â†’ **Actions**
2. Create new secret: `FACEBOOK_CONFIG`
3. Add this JSON configuration:

```json
{
  "facebook_page": "https://www.facebook.com/kuenselonline",
  "max_posts": 8,
  "delay_range": [2, 5],
  "headless": true,
  "timeout": 30
}
```

### ğŸ¯ That's it! 
The scraper will now run automatically every 6 hours and update your GitHub Pages site.

## ğŸ› ï¸ Manual Operations

### Local Development
```bash
# Copy the example config for local testing
cp config/config.example.json config/config.json
# Edit config/config.json with your configuration
```

### Workflow Management
```bash
# Enhanced cleanup script
./scripts/enhanced_cleanup.sh

# Manual trigger specific workflows
gh workflow run facebook-scraper.yml
gh workflow run cleanup.yml
gh workflow run repository-maintenance.yml
```

## ğŸ“ Project Structure

The project is now organized with a clean, professional structure:

```
ğŸ“ Facebook_Scrapper/
â”œâ”€â”€ ğŸ“ src/           # Python source code
â”œâ”€â”€ ğŸ“ scripts/       # Shell scripts & utilities  
â”œâ”€â”€ ğŸ“ config/        # Configuration files
â”œâ”€â”€ ğŸ“ web/           # Web interface files
â”œâ”€â”€ ğŸ“ docs/          # Documentation
â”œâ”€â”€ ğŸ“ data/          # Scraped data storage
â”œâ”€â”€ ğŸ“ log/           # Application logs
â””â”€â”€ ğŸ“ static_api/    # Generated API files
```

See [PROJECT_STRUCTURE.md](PROJECT_STRUCTURE.md) for detailed organization.

## ğŸ§¹ Repository Cleanup & Management

### Automated Cloud Workflows

| Workflow | Purpose | Schedule |
|----------|---------|----------|
| **Smart Workflow Deletion** | Intelligent cleanup based on patterns | Manual trigger only |
| **Repository Maintenance** | Daily optimization and health checks | Daily at 1 AM UTC |
| **Advanced Cleanup** | Bi-weekly comprehensive cleanup | Wed/Sun at 2 AM UTC |

### Manual Cleanup Options
Use the enhanced cleanup script for immediate control:

```bash
./enhanced_cleanup.sh
```

Features:
- ğŸ§  **Smart cleanup** - Removes old failed/cancelled runs intelligently
- ğŸ“… **Date-based** - Clean by age (7, 14, 30 days)  
- ğŸ¯ **Status-based** - Target specific run statuses
- ğŸ”¢ **Count-based** - Keep only N most recent runs
- â˜¢ï¸ **Nuclear option** - Emergency cleanup (use with caution!)
- ğŸ“Š **Statistics** - Detailed analytics before/after cleanup

## ğŸ”§ Architecture

### Cloud Automation Flow
```
GitHub Actions (Every 6 hours)
    â†“
ğŸŒ Chrome Browser (Ubuntu)
    â†“  
ğŸ•·ï¸ Scrape Kuensel Facebook
    â†“
ğŸ“Š Generate Static API
    â†“
ğŸ“¤ Update GitHub Repository
    â†“
ğŸš€ Deploy to GitHub Pages
```

### File Structure
```
â”œâ”€â”€ .github/workflows/          # Cloud automation workflows
â”‚   â”œâ”€â”€ facebook-scraper.yml   # Main scraping automation
â”‚   â”œâ”€â”€ cleanup.yml            # Advanced workflow cleanup
â”‚   â”œâ”€â”€ repository-maintenance.yml  # Daily maintenance
â”‚   â””â”€â”€ smart-workflow-deletion.yml # Manual cleanup tool
â”œâ”€â”€ data/                      # Scraped data storage
â”œâ”€â”€ static_api/               # Generated API files
â”œâ”€â”€ log/                      # Application logs
â”œâ”€â”€ enhanced_cleanup.sh       # Advanced cleanup script
â””â”€â”€ facebook_scrapper.py      # Core scraping engine
```

## ğŸš€ How It Works
1. **Automation**: macOS launch agent runs the scraper every hour
2. **Data Collection**: Selenium WebDriver scrapes Kuensel's Facebook page
3. **Smart Storage**: New posts are added to a single master file (no duplicates)
4. **Static Generation**: JSON API files are created in `static_api/` directory
5. **Display**: Frontend loads data from static files for fast, serverless operation

## File Structure
```
â”œâ”€â”€ frontend.html              # Main web interface
â”œâ”€â”€ facebook_scrapper.py       # Core scraper script
â”œâ”€â”€ generate_static_api.py     # Static API file generator
â”œâ”€â”€ view_posts.sh             # Local viewer launcher
â”œâ”€â”€ com.facebook.scraper.plist # Launch agent configuration
â”œâ”€â”€ static_api/               # Generated API files
â”‚   â”œâ”€â”€ posts.json
â”‚   â”œâ”€â”€ categories.json
â”‚   â”œâ”€â”€ stats.json
â”‚   â””â”€â”€ posts_general.json
â””â”€â”€ data/                     # Scraped data storage
    â”œâ”€â”€ kuensel_posts_master.json  # Master file with all posts
    â””â”€â”€ last_run.txt              # Tracks last scraping time
```

## GitHub Pages Setup
This repository is configured to automatically deploy to GitHub Pages, updating hourly with new posts.

## Requirements
- Python 3.7+
- Chrome/Chromium browser
- macOS (for launch agent automation)

## Dependencies
```bash
pip install -r requirements.txt
```

## ğŸ”’ Security Notice
- `config.json` contains sensitive credentials and is excluded from the repository
- Copy `config.example.json` to `config.json` and add your credentials
- Never commit `config.json` to version control
