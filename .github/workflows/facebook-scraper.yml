name: Facebook Scraper - Cloud Automation

on:
  schedule:
    # Optimized schedule - 4 times per day at strategic times
    - cron: '30 2 * * *'   # 8:30 AM Bhutan Time (morning peak)
    - cron: '30 6 * * *'   # 12:30 PM Bhutan Time (lunch peak)  
    - cron: '30 10 * * *'  # 4:30 PM Bhutan Time (evening peak)
    - cron: '30 16 * * *'  # 10:30 PM Bhutan Time (night update)
  workflow_dispatch:
    inputs:
      max_posts:
        description: 'Maximum posts to scrape'
        required: false
        default: '8'
        type: string
      force_scrape:
        description: 'Force scrape (ignore cooldown)'
        required: false
        default: false
        type: boolean

permissions:
  contents: write
  actions: write
  pages: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 15  # Prevent hanging
    
    steps:
    - name: 🚀 Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        
    - name: ⚡ Setup Python with cache
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'
        
    - name: 📦 Install dependencies (cached)
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: 🌐 Setup Chrome (optimized)
      run: |
        # Install Chrome efficiently
        wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        echo "deb http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee -a /etc/apt/sources.list.d/google.list
        sudo apt-get update -qq
        sudo apt-get install -y google-chrome-stable xvfb
        
    - name: ⚙️ Prepare configuration
      run: |
        if [ -z "${{ secrets.FACEBOOK_CONFIG }}" ]; then
          echo "⚠️  FACEBOOK_CONFIG secret not set, using example config"
          cp config/config.example.json config/config.json
        else
          echo "✅ Using secret configuration"
          echo '${{ secrets.FACEBOOK_CONFIG }}' > config/config.json
        fi
        
        # Show config (without sensitive data)
        echo "📋 Configuration loaded:"
        jq -r 'keys[]' config/config.json | head -5
        
    - name: 🕷️ Execute Facebook scraping
      run: |
        export DISPLAY=:99
        Xvfb :99 -ac -screen 0 1280x1024x24 &
        sleep 3
        
        # Handle cooldown based on input
        if [ "${{ github.event.inputs.force_scrape }}" = "true" ]; then
          echo "🔥 Force scrape enabled - removing cooldown"
          rm -f data/last_run.txt
        fi
        
        # Set max posts from input or default
        MAX_POSTS="${{ github.event.inputs.max_posts || '8' }}"
        
        echo "🚀 Starting scraping (max posts: $MAX_POSTS)..."
        timeout 600s python src/facebook_scrapper.py --max-posts $MAX_POSTS --headless --force-scrape
        
        # Check if scraping was successful
        if [ $? -eq 0 ]; then
          echo "✅ Scraping completed successfully"
        else
          echo "⚠️  Scraping completed with warnings"
        fi
        
    - name: 📊 Generate optimized API
      run: |
        echo "🔄 Generating static API files..."
        python src/generate_static_api.py
        
        # Validate API generation
        if [ -f "static_api/posts.json" ]; then
          POST_COUNT=$(jq length static_api/posts.json)
          echo "✅ API generated with $POST_COUNT posts"
        else
          echo "⚠️  API generation may have failed"
        fi
        
    - name: 🧹 Mini cleanup (every 10th run)
      run: |
        # Simple cleanup to prevent accumulation
        RUN_NUMBER=${{ github.run_number }}
        
        if [ $((RUN_NUMBER % 10)) -eq 0 ]; then
          echo "🗑️  Performing mini cleanup (run #$RUN_NUMBER)..."
          
          # Clean old log files
          find log/ -name "*.log" -mtime +3 -delete 2>/dev/null || true
          
          # Clean Python cache
          find . -name "*.pyc" -delete 2>/dev/null || true
          find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true
          
          echo "✅ Mini cleanup completed"
        else
          echo "ℹ️  Mini cleanup not needed (run #$RUN_NUMBER)"
        fi
        
    - name: 📤 Smart commit and push
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "Facebook Scraper Bot"
        
        # Check for changes
        git add -A
        
        if git diff --staged --quiet; then
          echo "ℹ️  No new content to commit"
        else
          # Count new posts if possible
          NEW_POSTS=""
          if [ -f "static_api/posts.json" ]; then
            POST_COUNT=$(jq length static_api/posts.json 2>/dev/null || echo "unknown")
            NEW_POSTS=" ($POST_COUNT posts total)"
          fi
          
          # Create informative commit message
          TIMESTAMP=$(date '+%Y-%m-%d %H:%M UTC')
          BHUTAN_TIME=$(TZ='Asia/Thimphu' date '+%Y-%m-%d %I:%M %p BTT')
          
          git commit -m "🤖 Automated update: $TIMESTAMP - 📱 Scraped Kuensel Facebook posts$NEW_POSTS - 🕐 Bhutan time: $BHUTAN_TIME - 🚀 Run #${{ github.run_number }} - 🎯 Trigger: ${{ github.event_name }} - ✅ Static API updated"
          
          # Push with retry logic
          for attempt in 1 2 3; do
            if git push origin main; then
              echo "✅ Successfully pushed changes (attempt $attempt)"
              break
            else
              echo "⚠️  Push failed, retrying in 5s (attempt $attempt)"
              sleep 5
            fi
          done
        fi
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        
    - name: 🎯 Execution summary
      if: always()
      run: |
        echo "📋 Scraper Execution Summary"
        echo "============================"
        echo "📅 Completed: $(date '+%Y-%m-%d %H:%M:%S UTC')"
        echo "🏃 Run number: ${{ github.run_number }}"
        echo "👤 Triggered by: ${{ github.actor }}"
        echo "⚡ Event: ${{ github.event_name }}"
        echo "⏱️  Duration: ${{ job.status }}"
        
        # Check file sizes
        if [ -f "data/kuensel_posts_master.json" ]; then
          DATA_SIZE=$(du -h data/kuensel_posts_master.json | cut -f1)
          echo "💾 Master data: $DATA_SIZE"
        fi
        
        if [ -f "static_api/posts.json" ]; then
          API_SIZE=$(du -h static_api/posts.json | cut -f1)
          echo "🔗 API file: $API_SIZE"
        fi
        
        echo "✅ Facebook Scraper automation completed!"

    - name: Auto-cleanup old workflow runs
      run: |
        echo "Cleaning up old workflow runs..."
        
        # Simple cleanup using GitHub CLI (pre-installed on ubuntu-latest)
        echo "${{ secrets.GITHUB_TOKEN }}" | gh auth login --with-token
        
        # Keep only last 25 workflow runs (conservative cleanup)
        echo "Removing old workflow runs (keeping last 25)..."
        gh run list --limit 50 --json databaseId --jq '.[25:] | .[].databaseId' | head -20 | while read -r run_id; do
          if [ -n "$run_id" ]; then
            gh api repos/${{ github.repository }}/actions/runs/$run_id -X DELETE || echo "Could not delete run $run_id"
          fi
        done
        
        echo "Cleanup completed"
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      if: github.ref == 'refs/heads/main'
      
    - name: Summary
      run: |
        echo "Automation Summary:"
        echo "========================"
        echo "Scraper: Completed successfully"
        echo "API: Static files generated (images only)" 
        echo "Git: Changes committed and pushed"
        echo "Cleanup: Old workflow runs removed"
        echo ""
        echo "Smart Schedule Active:"
        echo "   Morning (8:30AM-6:30PM): 6 runs/day"
        echo "   Evening (10:30PM): 1 run/day" 
        echo "   Night (2:30AM): 1 run/day"
        echo ""
        echo "Filter: Only posts with images included"
        echo "View results: https://${{ github.repository_owner }}.github.io/Facebook_Scrapper"
