name: Facebook Scraper - Cloud Automation

on:
  schedule:
    # Optimized schedule - 4 times per day at strategic times
    - cron: '30 2 * * *'   # 8:30 AM Bhutan Time (morning peak)
    - cron: '30 6 * * *'   # 12:30 PM Bhutan Time (lunch peak)  
    - cron: '30 10 * * *'  # 4:30 PM Bhutan Time (evening peak)
    - cron: '30 16 * * *'  # 10:30 PM Bhutan Time (night update)
  workflow_dispatch:
    inputs:
      max_posts:
        description: 'Maximum posts to scrape'
        required: false
        default: '8'
        type: string
      force_scrape:
        description: 'Force scrape (ignore cooldown)'
        required: false
        default: false
        type: boolean

permissions:
  contents: write
  actions: write
  pages: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 20  # Increased from 15 to 20 minutes
    
    steps:
    - name: üöÄ Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        
    - name: ‚ö° Setup Python with cache
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'
        
    - name: üì¶ Install dependencies (cached)
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: üåê Setup Chrome (optimized)
      run: |
        # Install Chrome efficiently
        wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        echo "deb http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee -a /etc/apt/sources.list.d/google.list
        sudo apt-get update -qq
        sudo apt-get install -y google-chrome-stable xvfb
        
        # Remove any existing chromedriver to avoid conflicts
        sudo rm -f /usr/bin/chromedriver /usr/local/bin/chromedriver
        
        # Verify Chrome installation
        echo "‚úÖ Chrome version: $(google-chrome --version)"
        echo "üì¶ webdriver-manager will handle ChromeDriver automatically"
        
    - name: ‚öôÔ∏è Prepare configuration
      run: |
        if [ -z "${{ secrets.FACEBOOK_CONFIG }}" ]; then
          echo "‚ö†Ô∏è  FACEBOOK_CONFIG secret not set, using example config"
          cp config/config.example.json config/config.json
          # Optimize for GitHub Actions performance
          echo "‚öôÔ∏è  Setting GitHub Actions optimized config"
          jq '.scraping.target_count = 5 | .process_facebook_photos = false | .download_images = false' config/config.json > config/config_tmp.json && mv config/config_tmp.json config/config.json
        else
          echo "‚úÖ Using secret configuration"
          echo '${{ secrets.FACEBOOK_CONFIG }}' > config/config.json
          # Ensure photo processing is disabled in GitHub Actions for performance
          jq '.process_facebook_photos = false | .download_images = false' config/config.json > config/config_tmp.json && mv config/config_tmp.json config/config.json
        fi
        
        # Show config (without sensitive data)
        echo "üìã Configuration loaded:"
        jq -r 'keys[]' config/config.json | head -5
        
    - name: üï∑Ô∏è Execute Facebook scraping
      run: |
        # Set up X11 display with reduced warnings
        export DISPLAY=:99
        export XKBPATH=/usr/share/X11/xkb
        Xvfb :99 -ac -screen 0 1280x1024x24 -noreset +extension GLX +render -logfile /tmp/xvfb.log 2>&1 &
        XVFB_PID=$!
        sleep 3
        
        # Handle cooldown based on input
        if [ "${{ github.event.inputs.force_scrape }}" = "true" ]; then
          echo "üî• Force scrape enabled - removing cooldown"
          rm -f data/last_run.txt
        fi
        
        # Set max posts from input or default
        MAX_POSTS="${{ github.event.inputs.max_posts || '8' }}"
        
        echo "üöÄ Starting scraping (max posts: $MAX_POSTS)..."
        
        # For scheduled runs, always force scrape to bypass cooldown
        # Increased timeout to 18 minutes (1080s) to handle Facebook's slow loading
        if [ "${{ github.event_name }}" = "schedule" ]; then
          echo "üïí Scheduled run detected - forcing scrape to bypass cooldown"
          timeout 1080s python src/facebook_scrapper.py --max-posts $MAX_POSTS --headless --force-scrape
        else
          timeout 1080s python src/facebook_scrapper.py --max-posts $MAX_POSTS --headless
        fi
        
        # Clean up Xvfb process
        kill $XVFB_PID 2>/dev/null || true
        
        # Check if scraping was successful
        if [ $? -eq 0 ]; then
          echo "‚úÖ Scraping completed successfully"
        else
          echo "‚ö†Ô∏è  Scraping completed with warnings"
        fi
        
    - name: üìä Generate optimized API
      run: |
        echo "üîÑ Generating static API files..."
        python src/generate_static_api.py
        
        # Validate API generation
        if [ -f "static_api/posts.json" ]; then
          POST_COUNT=$(jq length static_api/posts.json)
          echo "‚úÖ API generated with $POST_COUNT posts"
        else
          echo "‚ö†Ô∏è  API generation may have failed"
        fi
        
    - name: üßπ Mini cleanup (every 10th run)
      run: |
        # Simple cleanup to prevent accumulation
        RUN_NUMBER=${{ github.run_number }}
        
        if [ $((RUN_NUMBER % 10)) -eq 0 ]; then
          echo "üóëÔ∏è  Performing mini cleanup (run #$RUN_NUMBER)..."
          
          # Clean old log files
          find log/ -name "*.log" -mtime +3 -delete 2>/dev/null || true
          
          # Clean Python cache
          find . -name "*.pyc" -delete 2>/dev/null || true
          find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true
          
          echo "‚úÖ Mini cleanup completed"
        else
          echo "‚ÑπÔ∏è  Mini cleanup not needed (run #$RUN_NUMBER)"
        fi
        
    - name: üì§ Smart commit and push
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "Facebook Scraper Bot"
        
        # Check for changes
        git add -A
        
        if git diff --staged --quiet; then
          echo "‚ÑπÔ∏è  No new content to commit"
        else
          # Count new posts if possible
          NEW_POSTS=""
          if [ -f "static_api/posts.json" ]; then
            POST_COUNT=$(jq length static_api/posts.json 2>/dev/null || echo "unknown")
            NEW_POSTS=" ($POST_COUNT posts total)"
          fi
          
          # Create informative commit message
          TIMESTAMP=$(date '+%Y-%m-%d %H:%M UTC')
          BHUTAN_TIME=$(TZ='Asia/Thimphu' date '+%Y-%m-%d %I:%M %p BTT')
          
          git commit -m "ü§ñ Automated update: $TIMESTAMP - üì± Scraped Kuensel Facebook posts$NEW_POSTS - üïê Bhutan time: $BHUTAN_TIME - üöÄ Run #${{ github.run_number }} - üéØ Trigger: ${{ github.event_name }} - ‚úÖ Static API updated"
          
          # Push with retry logic
          for attempt in 1 2 3; do
            if git push origin main; then
              echo "‚úÖ Successfully pushed changes (attempt $attempt)"
              break
            else
              echo "‚ö†Ô∏è  Push failed, retrying in 5s (attempt $attempt)"
              sleep 5
            fi
          done
        fi
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        
    - name: üéØ Execution summary
      if: always()
      run: |
        echo "üìã Scraper Execution Summary"
        echo "============================"
        echo "üìÖ Completed: $(date '+%Y-%m-%d %H:%M:%S UTC')"
        echo "üèÉ Run number: ${{ github.run_number }}"
        echo "üë§ Triggered by: ${{ github.actor }}"
        echo "‚ö° Event: ${{ github.event_name }}"
        echo "‚è±Ô∏è  Duration: ${{ job.status }}"
        
        # Check file sizes
        if [ -f "data/kuensel_posts_master.json" ]; then
          DATA_SIZE=$(du -h data/kuensel_posts_master.json | cut -f1)
          echo "üíæ Master data: $DATA_SIZE"
        fi
        
        if [ -f "static_api/posts.json" ]; then
          API_SIZE=$(du -h static_api/posts.json | cut -f1)
          echo "üîó API file: $API_SIZE"
        fi
        
        echo "‚úÖ Facebook Scraper automation completed!"

    - name: Auto-cleanup old workflow runs
      run: |
        echo "üßπ Cleaning up old workflow runs..."
        
        # Authenticate with GitHub CLI (suppress informational messages)
        echo "${{ secrets.GITHUB_TOKEN }}" | gh auth login --with-token 2>/dev/null || true
        
        # Keep only last 30 workflow runs (conservative cleanup)
        echo "üìä Removing old workflow runs (keeping last 30)..."
        
        # Get runs to delete and count them
        RUNS_TO_DELETE=$(gh run list --limit 50 --json databaseId --jq '.[30:] | .[].databaseId' 2>/dev/null || echo "")
        
        if [ -n "$RUNS_TO_DELETE" ]; then
          echo "$RUNS_TO_DELETE" | head -15 | while read -r run_id; do
            if [ -n "$run_id" ]; then
              gh api repos/${{ github.repository }}/actions/runs/$run_id -X DELETE 2>/dev/null && echo "üóëÔ∏è  Deleted run $run_id" || echo "‚ö†Ô∏è  Could not delete run $run_id"
            fi
          done
        else
          echo "‚ÑπÔ∏è  No old runs to delete"
        fi
        
        echo "‚úÖ Cleanup completed"
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      if: github.ref == 'refs/heads/main'
      
    - name: Summary
      run: |
        echo "Automation Summary:"
        echo "========================"
        echo "Scraper: Completed successfully"
        echo "API: Static files generated (images only)" 
        echo "Git: Changes committed and pushed"
        echo "Cleanup: Old workflow runs removed"
        echo ""
        echo "Smart Schedule Active:"
        echo "   Morning (8:30AM-6:30PM): 6 runs/day"
        echo "   Evening (10:30PM): 1 run/day" 
        echo "   Night (2:30AM): 1 run/day"
        echo ""
        echo "Filter: Only posts with images included"
        echo "View results: https://${{ github.repository_owner }}.github.io/Facebook_Scrapper"
